
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>激活函数的作用及部分激活函数介绍</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab title="激活函数的作用及部分激活函数介绍"
                  environment="web"
                  feedback-link="http://www.sctu.edu.cn">
    
      <google-codelab-step label="问题提出" duration="0">
        <p>这周在学习前向传播和神经网络全连接的时候，激活函数这部分一直困扰着我，不能明白其原理作用，于是在网上查阅了很大资料，实验总结之后做了笔记，方便大家学习。</p>


      </google-codelab-step>
    
      <google-codelab-step label="问题的分析" duration="0">
        <p>学习深度学习算法，一般都是先学线性回归，y=wx+b。线性回归理论可以很简单与其他算法思想相结合，但是线性回归也存在着很多局限性，在实际应用中一个很大的局限就是&#39;线性&#39;。而在实际运用中遇到的比较多且比较难的问题一般是非线性问题。<br>来看个例子：</p>
<pre><code>一个二分类问题，请对下方图片中的图形进行分类

如果只是单纯的以一次线性回归Y1=w1*x+b1来做分类，几乎不可能从图中找出一条直线来区分两个图形。

就算多叠加几层线性回归，仍然不理想。
</code></pre>
<p>$$  Y2=w1*Y1+b2 $$<br>$$  Y3=w3*Y2+b1 $$</p>
<p><br><br>图一<br></p>


      </google-codelab-step>
    
      <google-codelab-step label="问题的解决" duration="0">
        <p>通过图一可以看出，线性回归其实已经能区分一部分图形，只是效果还不能让人满意。既然直线的效果不好，能不能用曲线呢？或者能不能让图一中的直线弯曲来达到我们想要的效果呢？</p>
<p>答案是能，这也是激活函数一个很重要的作用。</p>
<p>激活函数–Sigmiod函数：<br> $$<br> f(x)=\frac{1}{1+e^{-x}}<br> $$<br></p>
<p>图二 Sigmiod函数<br></p>
<p>给每层函数加入激活函数，f(Y1)、f(Y2)，让线性变为非线性。</p>
<p><br><br>图三<br></p>
<p>或许你还会提出一个问题，直线是变成曲线了，但怎么确保这条曲线就是我想要的呢？</p>
<p>将原函数加入激活函数，原函数不一定是变成激活函数那种曲线，每层隐藏层都加入了激活函数，在每一次迭代的时候，会根据训练效果来决定这条线哪该多弯一点哪该少弯一点。</p>


      </google-codelab-step>
    
      <google-codelab-step label="部分激活函数介绍" duration="0">
        <p>激活函数的作用如果只是线性和非线性的转换，显然不用设计这么多激活函数。非线性变换只是作为激活函数的一个基础作用，不同激活函数还拥有一些额外的作用。</p>
<p>下面介绍一些常用的激活函数及其其他的作用。</p>
<ol type="1">
<li>rule函数</li>
</ol>
<p><br><br>图四：rule函数<br></p>
<pre><code>a.非线性变换。
b.原函数小于0部分全变为0，大于0部分保留原值。
c.优点：简单，计算机计算速度快，在正区间内可以较好的避免梯度消失的问题。
</code></pre>
<ol type="1">
<li>sigmiod函数<br>$$ f(x)=\frac{1}{1+e^{-x}}$$<br></li>
</ol>
<p>图五：sigmiod函数<br></p>
<pre><code>a. 非线性变换。
b. 值压缩，将原函数中的值投影（压缩）在0-1之间。
c.容易产生梯度消失问题。
</code></pre>
<ol type="1">
<li>softmax函数</li>
</ol>
<p>函数的输入是从K个不同的线性函数得到的结果，而样本向量 x 属于第 j 个分类的概率为：</p>
<p>$$<br>p(y=j)=\frac{e^{x^t}W<em>j}{\sum</em>{k=1}^K e^{x^t W_k}}<br>$$</p>
<p>图六：softmax函数<br></p>
<pre><code>a. 非线性变换。
b. 值压缩，将原函数中的值投影（压缩）在0-1之间。
c. 常应用于多分类问题的概率计算，假设有一个4分类问题，用softmax函数得出的属于某个分类的概率[x1,x2,x3,x4]，x1,x2,x3,x4全位于0到1，且x1+x2+x3+x4=1。
</code></pre>
<ol type="1">
<li>tanh函数</li>
</ol>
<p>$$<br>tanh⁡(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}<br>$$</p>
<p>图七：tanh函数<br></p>
<pre><code>a. 非线性变换。
b. 值压缩，将原函数中的值投影（压缩）在-1到1之间。
c.存在梯度消失问题，运算较复杂。
</code></pre>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>

</body>
</html>
